{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance with and without the bright-dark filter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we shall compare the performance on the basic model (ran on just one epoch) with images which have been resized against images which have been resized and had the bright-dark filter applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained on resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the necessary packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(42)\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to replicate this notebook, change the paths below to the file paths where the train and test folders are stored on your desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:\\\\Users\\\\Team Knowhow\\\\Documents\\\\YEAR 4\\\\Data Science Toolbox\\\\DST-NN-Project\\\\Erin Pollard\"\n",
    "train_path = \"C:\\\\Users\\\\Team Knowhow\\\\Documents\\\\YEAR 4\\\\Data Science Toolbox\\\\DST-NN-Project\\\\Erin Pollard\\\\Train\"\n",
    "test_path = \"C:\\\\Users\\\\Team Knowhow\\\\Documents\\\\YEAR 4\\\\Data Science Toolbox\\\\DST-NN-Project\\\\Erin Pollard\\\\Test\"\n",
    "\n",
    "# Resizing the images to 30x30x3\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CATEGORIES = len(os.listdir(train_path))\n",
    "NUM_CATEGORIES = NUM_CATEGORIES \n",
    "NUM_CATEGORIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the pixels for each image are stored in a ndarray, the same is done for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3) (39209,)\n"
     ]
    }
   ],
   "source": [
    "image_data = []\n",
    "image_labels = []\n",
    "\n",
    "for i in range(NUM_CATEGORIES):\n",
    "    path = data_dir + '/Train/' + str(i)\n",
    "    images = os.listdir(path)\n",
    "\n",
    "    for img in images:\n",
    "        try:\n",
    "            image = cv2.imread(path + '/' + img)\n",
    "            image_fromarray = Image.fromarray(image, 'RGB')\n",
    "            resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "            image_data.append(np.array(resize_image))\n",
    "            image_labels.append(i)\n",
    "        except:\n",
    "            print(\"Error in \" + img)\n",
    "\n",
    "# Changing the list to numpy array\n",
    "image_data = np.array(image_data)\n",
    "image_labels = np.array(image_labels)\n",
    "\n",
    "print(image_data.shape, image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39209, 32, 32, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we randomly shuffle the pixel and label ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shuffle_indexes = np.arange(image_data.shape[0])\n",
    "np.random.shuffle(shuffle_indexes)\n",
    "image_data = image_data[shuffle_indexes]\n",
    "image_labels = image_labels[shuffle_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 15, 13, ..., 39,  1, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the train validation split is performed. Note, the pixel values are normalised to be between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (27446, 32, 32, 3)\n",
      "X_valid.shape (11763, 32, 32, 3)\n",
      "y_train.shape (27446,)\n",
      "y_valid.shape (11763,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(image_data, image_labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train/255 \n",
    "X_val = X_val/255\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_valid.shape\", X_val.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code converts the labels by one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27446, 43)\n",
      "(11763, 43)\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, NUM_CATEGORIES)\n",
    "y_val = keras.utils.to_categorical(y_val, NUM_CATEGORIES)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the structure of the model is specified, e.g., the number of layers, number of neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([    \n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(IMG_HEIGHT,IMG_WIDTH,channels)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    keras.layers.Dense(43, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimiser is defined here, as well as the number of epochs (which is specified as 1 for time-saving purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 1\n",
    "\n",
    "opt = tf.keras.optimizers.legacy.Adam(lr=lr, decay=lr / (epochs * 0.5))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the model is fitted, but not before data augmentation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858/858 [==============================] - 134s 141ms/step - loss: 1.0815 - accuracy: 0.7084 - val_loss: 0.0885 - val_accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "history = model.fit(aug.flow(X_train, y_train, batch_size=32), epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the test data is prepared and the model is used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + '/Test.csv')\n",
    "\n",
    "labels = test[\"ClassId\"].values\n",
    "imgs = test[\"Path\"].values\n",
    "\n",
    "data =[]\n",
    "\n",
    "for img in imgs:\n",
    "    try:\n",
    "        image = cv2.imread(data_dir + '/' +img)\n",
    "        image_fromarray = Image.fromarray(image, 'RGB')\n",
    "        resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "        data.append(np.array(resize_image))\n",
    "    except:\n",
    "        print(\"Error in \" + img)\n",
    "X_test = np.array(data)\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 14s 34ms/step\n",
      "Test Data accuracy:  93.4916864608076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',accuracy_score(labels, pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.58      0.74        60\n",
      "           1       0.94      0.98      0.96       720\n",
      "           2       0.93      0.97      0.95       750\n",
      "           3       0.98      0.81      0.89       450\n",
      "           4       0.99      0.95      0.97       660\n",
      "           5       0.85      0.96      0.90       630\n",
      "           6       0.94      0.95      0.95       150\n",
      "           7       0.93      0.89      0.91       450\n",
      "           8       0.86      1.00      0.92       450\n",
      "           9       0.94      1.00      0.97       480\n",
      "          10       1.00      0.95      0.97       660\n",
      "          11       0.96      0.88      0.92       420\n",
      "          12       0.98      0.95      0.96       690\n",
      "          13       1.00      0.99      0.99       720\n",
      "          14       1.00      1.00      1.00       270\n",
      "          15       0.99      1.00      0.99       210\n",
      "          16       1.00      0.99      0.99       150\n",
      "          17       1.00      0.97      0.98       360\n",
      "          18       0.88      0.91      0.89       390\n",
      "          19       1.00      0.63      0.78        60\n",
      "          20       0.77      0.96      0.86        90\n",
      "          21       0.76      0.67      0.71        90\n",
      "          22       1.00      0.93      0.97       120\n",
      "          23       0.85      1.00      0.92       150\n",
      "          24       0.86      0.81      0.83        90\n",
      "          25       0.99      0.96      0.97       480\n",
      "          26       0.87      0.91      0.89       180\n",
      "          27       0.76      0.43      0.55        60\n",
      "          28       0.80      0.93      0.86       150\n",
      "          29       0.55      0.98      0.71        90\n",
      "          30       0.97      0.48      0.64       150\n",
      "          31       0.83      1.00      0.91       270\n",
      "          32       1.00      0.65      0.79        60\n",
      "          33       0.84      1.00      0.91       210\n",
      "          34       0.99      1.00      1.00       120\n",
      "          35       0.99      0.92      0.96       390\n",
      "          36       1.00      0.93      0.96       120\n",
      "          37       0.94      0.98      0.96        60\n",
      "          38       0.98      0.98      0.98       690\n",
      "          39       1.00      0.67      0.80        90\n",
      "          40       0.96      0.98      0.97        90\n",
      "          41       0.61      0.63      0.62        60\n",
      "          42       1.00      0.87      0.93        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.92      0.88      0.89     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained on resized and bright-dark filter images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the bright_dark filter is defined. It increases the brightness of dark images and darkens those which are too bright (perhaps due to camera flash). Note, the pixel values are normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bright_dark(pixels):\n",
    "  global cnp\n",
    "  pixels = pixels.astype('float32')\n",
    "  means = pixels.mean(axis=(0,1), dtype='float64')\n",
    "  #calculate weighted average\n",
    "  weighted = means[0]*0.21 + means[1]*0.72 + means[2]*0.07\n",
    "  if weighted >= 175: #darken\n",
    "    centered_pixels = pixels - means\n",
    "    negative_min = abs(centered_pixels.min())\n",
    "    shifted_pixels = centered_pixels + negative_min\n",
    "    cnp = shifted_pixels/(shifted_pixels.max())\n",
    "    # confirm it had the desired effect\n",
    "    means = cnp.mean(axis=(0,1), dtype='float64')\n",
    "  elif weighted <= 75: #brighten\n",
    "    centered_pixels = pixels + means\n",
    "    max_val = abs(centered_pixels.max())\n",
    "    cnp = centered_pixels/(max_val)\n",
    "    # confirm it had the desired effect\n",
    "    means = cnp.mean(axis=(0,1), dtype='float64')\n",
    "  else:\n",
    "    cnp = pixels/255\n",
    "  return cnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(NUM_CATEGORIES):\n",
    "    path = data_dir + '/Train/' + str(i)\n",
    "    images = os.listdir(path)\n",
    "\n",
    "    for img in images:\n",
    "        try:\n",
    "            image = cv2.imread(path + '/' + img)\n",
    "            image_from_array = Image.fromarray(image, 'RGB')\n",
    "            resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "            bd = bright_dark(np.array(resize_image))\n",
    "            data.append(bd)\n",
    "            labels.append(i)\n",
    "        except:\n",
    "            print(\"Error in \" + img)\n",
    "\n",
    "# Changing the list to numpy array\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shuffle_indexes = np.arange(data.shape[0])\n",
    "np.random.shuffle(shuffle_indexes)\n",
    "data = data[shuffle_indexes]\n",
    "labels = labels[shuffle_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (27446, 32, 32, 3)\n",
      "X_valid.shape (11763, 32, 32, 3)\n",
      "y_train.shape (27446,)\n",
      "y_valid.shape (11763,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train\n",
    "X_val = X_val\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_valid.shape\", X_val.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27446, 43)\n",
      "(11763, 43)\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, NUM_CATEGORIES)\n",
    "y_val = keras.utils.to_categorical(y_val, NUM_CATEGORIES)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858/858 [==============================] - 173s 202ms/step - loss: 4.1073 - accuracy: 0.0428 - val_loss: 3.5330 - val_accuracy: 0.0572\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(aug.flow(X_train, y_train, batch_size=32), epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the model's validation accuracy score is strikingly low (<6%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + '/Test.csv')\n",
    "\n",
    "t_labels = test[\"ClassId\"].values\n",
    "t_imgs = test[\"Path\"].values\n",
    "\n",
    "t_data =[]\n",
    "\n",
    "for img in t_imgs:\n",
    "    try:\n",
    "        image = cv2.imread(data_dir + '/' +img)\n",
    "        image_fromarray = Image.fromarray(image, 'RGB')\n",
    "        resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "        bd = bright_dark(np.array(resize_image))\n",
    "        t_data.append(bd)\n",
    "    except:\n",
    "        print(\"Error in \" + img)\n",
    "X_test = np.array(t_data)\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the predictions for the bright-dark images are compared with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 8s 19ms/step\n",
      "Test Data accuracy:  0.7125890736342043\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',accuracy_score(t_labels, pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data scores an accuracy of 0.71, which is significantly lower than the 93.49 accuracy on the unaltered images. Below demonstrates the classofication report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(t_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        60\n",
      "           1       0.00      0.00      0.00       720\n",
      "           2       0.00      0.00      0.00       750\n",
      "           3       0.00      0.00      0.00       450\n",
      "           4       0.00      0.00      0.00       660\n",
      "           5       0.00      0.00      0.00       630\n",
      "           6       0.00      0.00      0.00       150\n",
      "           7       0.00      0.00      0.00       450\n",
      "           8       0.00      0.00      0.00       450\n",
      "           9       0.00      0.00      0.00       480\n",
      "          10       0.00      0.00      0.00       660\n",
      "          11       0.00      0.00      0.00       420\n",
      "          12       0.00      0.00      0.00       690\n",
      "          13       0.00      0.00      0.00       720\n",
      "          14       0.00      0.00      0.00       270\n",
      "          15       0.00      0.00      0.00       210\n",
      "          16       0.00      0.00      0.00       150\n",
      "          17       0.00      0.00      0.00       360\n",
      "          18       0.00      0.00      0.00       390\n",
      "          19       0.00      0.00      0.00        60\n",
      "          20       0.00      0.00      0.00        90\n",
      "          21       0.00      0.00      0.00        90\n",
      "          22       0.00      0.00      0.00       120\n",
      "          23       0.00      0.00      0.00       150\n",
      "          24       0.00      0.00      0.00        90\n",
      "          25       0.00      0.00      0.00       480\n",
      "          26       0.00      0.00      0.00       180\n",
      "          27       0.00      0.00      0.00        60\n",
      "          28       0.00      0.00      0.00       150\n",
      "          29       0.00      0.00      0.00        90\n",
      "          30       0.00      0.00      0.00       150\n",
      "          31       0.00      0.00      0.00       270\n",
      "          32       0.00      0.00      0.00        60\n",
      "          33       0.00      0.00      0.00       210\n",
      "          34       0.00      0.00      0.00       120\n",
      "          35       0.00      0.00      0.00       390\n",
      "          36       0.00      0.00      0.00       120\n",
      "          37       0.00      0.00      0.00        60\n",
      "          38       0.00      0.00      0.00       690\n",
      "          39       0.00      0.00      0.00        90\n",
      "          40       0.00      0.00      0.00        90\n",
      "          41       0.00      0.00      0.00        60\n",
      "          42       0.01      1.00      0.01        90\n",
      "\n",
      "    accuracy                           0.01     12630\n",
      "   macro avg       0.00      0.02      0.00     12630\n",
      "weighted avg       0.00      0.01      0.00     12630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model trained on the unaltered images was superior to the bright-dark image model, we decided to not pursue this form of image preprocessing any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_env",
   "language": "python",
   "name": "hpc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
