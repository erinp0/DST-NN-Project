{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee93cc2",
   "metadata": {},
   "source": [
    "# Classifying adversarial images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc85c8",
   "metadata": {},
   "source": [
    "In order to aid in the correct classification of traffic signs, we wanted to explore designing a binary classifier which could distinguish between adversarial images and untampered images. In this notebook, the class label 1 represents an adversarial image and the class label 0 represents an untampered image.\n",
    "As the noisy image attacks were to be dealt with by the denoiser, they are not included in this notebook as adversary images. We thought their inclusion were worsen the model's ability to detect adversary images as they are dissimilar to the patch style attacks and therefore the model may fail to learn appropriate patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2ea2a",
   "metadata": {},
   "source": [
    "The idea would be to run images through this classifier first, before inputting the images into the multiclass CNN. The output from this binary classifier would serve as an additional feature for the multiclass CNN and would advise on whether the image it was classifying has been perturbed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7ab66",
   "metadata": {},
   "source": [
    "First, import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb595346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(42)\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b43242",
   "metadata": {},
   "source": [
    "Next, define the file paths for the adversarial images and real images. If you would like to replicate this notebook, the adversarial images can be found [here](https://drive.google.com/drive/u/0/folders/1B7XcUAJHHn1rkReNK7sENGZ61y_A8OY8). The untampered image npy file can be found [here](https://drive.google.com/drive/u/0/folders/19b7cLwvnNHwuOL3eQ0gaCBclAEKvD25B) and is called '32_filter_training_data.npy'. Once downloaded, you can edit the file paths below to match your desktop file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa248320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#untampered dataset (with bright dark filter applied), stored as npy files within folders\n",
    "untampered = np.load('C:\\\\Users\\\\Team Knowhow\\\\Documents\\\\YEAR 4\\\\Data Science Toolbox\\\\DST-NN-Project\\\\Erin Pollard\\\\32_filter_training_data.npy')\n",
    "\n",
    "#adversarial image dataset, stored as 3 npy files - 3 types of adversarial attacks\n",
    "adv_data2 = np.load('black_patched_training.npy')\n",
    "adv_data3 = np.load('white_patched_training.npy')\n",
    "adv_data4 = np.load('same_image_patched_training.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf62cc",
   "metadata": {},
   "source": [
    "Next, we want to create an array which stores the class labels. We shall store 117627 1's to the labels array and append 39,209 0's for the number of original images. We then combine the adversary and untampered ndarrays to form one training set, before shuffling both the training set and labels (using the same index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915ab0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones(117627,dtype='float32')\n",
    "zeros = np.zeros(39209,dtype='float32')\n",
    "labels = np.concatenate((ones,zeros),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2ae26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.concatenate((adv_data2,adv_data3,adv_data4,untampered),axis=0,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da43d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.arange(training_set.shape[0])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(s)\n",
    "training_set=training_set[s]\n",
    "labels=labels[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2de3d",
   "metadata": {},
   "source": [
    "Here we create the train-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c78af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (109785, 32, 32, 3)\n",
      "X_valid.shape (47051, 32, 32, 3)\n",
      "y_train.shape (109785,)\n",
      "y_valid.shape (47051,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(training_set, labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train\n",
    "X_val = X_val\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_valid.shape\", X_val.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafe41f6",
   "metadata": {},
   "source": [
    "Next we define the structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04d2123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 30, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 3, 3, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               590336    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 723,713\n",
      "Trainable params: 722,369\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([    \n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "\n",
    "    \n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(), #normalises output mean close to 0\n",
    "    keras.layers.Dropout(rate=0.4),\n",
    "    \n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db121716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 1\n",
    "\n",
    "opt =tf.keras.optimizers.legacy.Adam(lr=lr, decay=lr / (epochs * 0.5))\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3754b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3431/3431 [==============================] - 280s 80ms/step - loss: 0.4825 - accuracy: 0.7317 - val_loss: 0.3901 - val_accuracy: 0.7527\n"
     ]
    }
   ],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "history = model.fit(aug.flow(X_train, y_train, batch_size=32), epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870628e7",
   "metadata": {},
   "source": [
    "Below plots should only be ran if more than one epoch is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea844cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy \n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c5abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_env",
   "language": "python",
   "name": "hpc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
