{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance with and without the bright-dark filter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we shall compare the performance on the basic model (ran on just one epoch) with images which have been resized against images which have been resized and had the bright-dark filter applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained on resized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(42)\n",
    "\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to replicate this notebook, change the paths below to the file paths where the unedited train and test npy files are stored on your desktop. They can be downloaded from [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('32_original_train_data.npy')\n",
    "training_label = np.load('32_original_train_labels.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39209, 32, 32, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CATEGORIES = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the train validation split is performed. Note, the pixel values are normalised to be between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (27446, 32, 32, 3)\n",
      "X_valid.shape (11763, 32, 32, 3)\n",
      "y_train.shape (27446,)\n",
      "y_valid.shape (11763,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(training_data, training_label, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train/255 \n",
    "X_val = X_val/255\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_valid.shape\", X_val.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code converts the labels by one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27446, 43)\n",
      "(11763, 43)\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, NUM_CATEGORIES)\n",
    "y_val = keras.utils.to_categorical(y_val, NUM_CATEGORIES)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the structure of the model is specified, e.g., the number of layers, number of neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([    \n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    keras.layers.Dense(43, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimiser is defined here, as well as the number of epochs (which is specified as 1 for time-saving purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 1\n",
    "\n",
    "opt = tf.keras.optimizers.legacy.Adam(lr=lr, decay=lr / (epochs * 0.5))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the model is fitted, but not before data augmentation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858/858 [==============================] - 91s 104ms/step - loss: 1.0830 - accuracy: 0.7072 - val_loss: 0.1051 - val_accuracy: 0.9706\n"
     ]
    }
   ],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "history = model.fit(aug.flow(X_train, y_train, batch_size=32), epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the test data is prepared and the model is used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('32_original_test_data.npy')\n",
    "test_labels = np.load('32_original_test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  1, 38, ...,  6,  7, 10], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 9s 22ms/step\n",
      "Test Data accuracy:  93.04829770387965\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = test_data\n",
    "X_test = X_test/255\n",
    "\n",
    "pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',accuracy_score(test_labels, pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(test_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81        60\n",
      "           1       0.94      0.99      0.96       720\n",
      "           2       0.99      0.97      0.98       750\n",
      "           3       0.80      0.92      0.86       450\n",
      "           4       0.95      0.98      0.96       660\n",
      "           5       0.72      0.88      0.79       630\n",
      "           6       0.99      0.95      0.97       150\n",
      "           7       0.99      0.68      0.80       450\n",
      "           8       0.97      0.83      0.89       450\n",
      "           9       0.96      1.00      0.98       480\n",
      "          10       1.00      0.97      0.99       660\n",
      "          11       0.93      1.00      0.96       420\n",
      "          12       0.93      0.98      0.96       690\n",
      "          13       0.97      1.00      0.98       720\n",
      "          14       1.00      0.99      1.00       270\n",
      "          15       0.98      1.00      0.99       210\n",
      "          16       1.00      0.93      0.97       150\n",
      "          17       1.00      0.89      0.94       360\n",
      "          18       0.88      0.90      0.89       390\n",
      "          19       1.00      1.00      1.00        60\n",
      "          20       0.58      1.00      0.74        90\n",
      "          21       0.71      0.73      0.72        90\n",
      "          22       0.85      0.87      0.86       120\n",
      "          23       0.84      0.79      0.82       150\n",
      "          24       0.93      0.92      0.93        90\n",
      "          25       0.96      0.95      0.95       480\n",
      "          26       0.84      0.97      0.90       180\n",
      "          27       0.97      0.62      0.76        60\n",
      "          28       0.99      0.96      0.98       150\n",
      "          29       0.92      0.97      0.94        90\n",
      "          30       1.00      0.41      0.58       150\n",
      "          31       0.97      0.98      0.97       270\n",
      "          32       0.98      1.00      0.99        60\n",
      "          33       0.96      0.92      0.94       210\n",
      "          34       0.99      1.00      1.00       120\n",
      "          35       0.97      0.96      0.97       390\n",
      "          36       1.00      0.98      0.99       120\n",
      "          37       0.98      0.85      0.91        60\n",
      "          38       1.00      0.97      0.99       690\n",
      "          39       0.85      0.86      0.85        90\n",
      "          40       0.88      0.98      0.93        90\n",
      "          41       0.73      0.88      0.80        60\n",
      "          42       0.94      0.64      0.76        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.92      0.90      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trained on resized and bright-dark filter images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the bright_dark filter is defined. It increases the brightness of dark images and darkens those which are too bright (perhaps due to camera flash). Note, the pixel values are normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_training_data = np.load('32_filter_training_data.npy')\n",
    "bd_training_label = np.load('32_filter_training_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the bright-dark filter data has already been normalised to between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (27446, 32, 32, 3)\n",
      "X_valid.shape (11763, 32, 32, 3)\n",
      "y_train.shape (27446,)\n",
      "y_valid.shape (11763,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(bd_training_data, bd_training_label, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train\n",
    "X_val = X_val\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_valid.shape\", X_val.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27446, 43)\n",
      "(11763, 43)\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, NUM_CATEGORIES)\n",
    "y_val = keras.utils.to_categorical(y_val, NUM_CATEGORIES)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([    \n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.BatchNormalization(axis=-1),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.5),\n",
    "    \n",
    "    keras.layers.Dense(43, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Team Knowhow\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 1\n",
    "\n",
    "opt = tf.keras.optimizers.legacy.Adam(lr=lr, decay=lr / (epochs * 0.5))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858/858 [==============================] - 91s 104ms/step - loss: 1.0324 - accuracy: 0.7285 - val_loss: 0.0754 - val_accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "history2 = model.fit(aug.flow(X_train, y_train, batch_size=32), epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('32_filter_test_data.npy')\n",
    "test_labels = np.load('32_filter_test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.68627453, 0.54901963, 0.45490196],\n",
       "         [0.67450982, 0.53725493, 0.45490196],\n",
       "         [0.67843139, 0.5411765 , 0.46666667],\n",
       "         ...,\n",
       "         [0.57254905, 0.4627451 , 0.39215687],\n",
       "         [0.58039218, 0.47450981, 0.3882353 ],\n",
       "         [0.53333336, 0.43137255, 0.35294119]],\n",
       "\n",
       "        [[0.69411767, 0.55686277, 0.45490196],\n",
       "         [0.68627453, 0.54901963, 0.4509804 ],\n",
       "         [0.68627453, 0.5529412 , 0.45882353],\n",
       "         ...,\n",
       "         [0.69803923, 0.56470591, 0.47450981],\n",
       "         [0.68627453, 0.56078434, 0.47843137],\n",
       "         [0.67843139, 0.55686277, 0.47843137]],\n",
       "\n",
       "        [[0.68235296, 0.55686277, 0.45882353],\n",
       "         [0.68627453, 0.5529412 , 0.45882353],\n",
       "         [0.67843139, 0.54901963, 0.44705883],\n",
       "         ...,\n",
       "         [0.70588237, 0.56470591, 0.47058824],\n",
       "         [0.7019608 , 0.56470591, 0.47843137],\n",
       "         [0.69803923, 0.56078434, 0.47058824]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.65882355, 0.53725493, 0.45882353],\n",
       "         [0.64705884, 0.52941179, 0.45490196],\n",
       "         [0.64313728, 0.52549022, 0.45882353],\n",
       "         ...,\n",
       "         [0.66274512, 0.53725493, 0.4509804 ],\n",
       "         [0.65882355, 0.53333336, 0.45490196],\n",
       "         [0.67058825, 0.5411765 , 0.4627451 ]],\n",
       "\n",
       "        [[0.65098041, 0.53333336, 0.45490196],\n",
       "         [0.65490198, 0.52549022, 0.44705883],\n",
       "         [0.65098041, 0.51764709, 0.44705883],\n",
       "         ...,\n",
       "         [0.65882355, 0.52941179, 0.44705883],\n",
       "         [0.65098041, 0.53333336, 0.45882353],\n",
       "         [0.65490198, 0.54509807, 0.4509804 ]],\n",
       "\n",
       "        [[0.65490198, 0.52941179, 0.43921569],\n",
       "         [0.64705884, 0.52549022, 0.43529412],\n",
       "         [0.64705884, 0.52941179, 0.4627451 ],\n",
       "         ...,\n",
       "         [0.65098041, 0.5411765 , 0.4509804 ],\n",
       "         [0.65490198, 0.5411765 , 0.45882353],\n",
       "         [0.66666669, 0.54901963, 0.44705883]]],\n",
       "\n",
       "\n",
       "       [[[0.23921569, 0.27843139, 0.23921569],\n",
       "         [0.24705882, 0.3137255 , 0.34509805],\n",
       "         [0.24705882, 0.3137255 , 0.36470589],\n",
       "         ...,\n",
       "         [0.26274511, 0.25098041, 0.3019608 ],\n",
       "         [0.26274511, 0.25490198, 0.27843139],\n",
       "         [0.27058825, 0.26666668, 0.29019609]],\n",
       "\n",
       "        [[0.22745098, 0.27450982, 0.23921569],\n",
       "         [0.25490198, 0.32549021, 0.3764706 ],\n",
       "         [0.24313726, 0.3137255 , 0.38039216],\n",
       "         ...,\n",
       "         [0.26274511, 0.25098041, 0.30588236],\n",
       "         [0.27450982, 0.27058825, 0.36862746],\n",
       "         [0.26666668, 0.26666668, 0.34117648]],\n",
       "\n",
       "        [[0.23137255, 0.27450982, 0.22745098],\n",
       "         [0.26274511, 0.32941177, 0.36078432],\n",
       "         [0.24705882, 0.31764707, 0.37254903],\n",
       "         ...,\n",
       "         [0.25490198, 0.25490198, 0.27843139],\n",
       "         [0.27058825, 0.26666668, 0.31764707],\n",
       "         [0.27450982, 0.27058825, 0.3137255 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.17647059, 0.19215687, 0.19607843],\n",
       "         [0.23137255, 0.26666668, 0.3137255 ],\n",
       "         [0.21960784, 0.25882354, 0.3137255 ],\n",
       "         ...,\n",
       "         [0.25098041, 0.25098041, 0.27058825],\n",
       "         [0.25882354, 0.25882354, 0.30980393],\n",
       "         [0.26666668, 0.25882354, 0.29803923]],\n",
       "\n",
       "        [[0.18431373, 0.19215687, 0.2       ],\n",
       "         [0.24313726, 0.26274511, 0.3137255 ],\n",
       "         [0.22745098, 0.25490198, 0.30588236],\n",
       "         ...,\n",
       "         [0.23921569, 0.23529412, 0.23529412],\n",
       "         [0.25882354, 0.25490198, 0.26666668],\n",
       "         [0.26274511, 0.26274511, 0.28235295]],\n",
       "\n",
       "        [[0.18431373, 0.18431373, 0.20392157],\n",
       "         [0.24313726, 0.25882354, 0.30980393],\n",
       "         [0.22745098, 0.25882354, 0.30980393],\n",
       "         ...,\n",
       "         [0.24313726, 0.23137255, 0.24313726],\n",
       "         [0.27843139, 0.26274511, 0.28627452],\n",
       "         [0.27450982, 0.27450982, 0.29411766]]],\n",
       "\n",
       "\n",
       "       [[[0.14901961, 0.15686275, 0.20392157],\n",
       "         [0.14509805, 0.15294118, 0.2       ],\n",
       "         [0.14509805, 0.15294118, 0.19215687],\n",
       "         ...,\n",
       "         [0.18431373, 0.17254902, 0.20784314],\n",
       "         [0.1882353 , 0.17647059, 0.21176471],\n",
       "         [0.1882353 , 0.18039216, 0.21568628]],\n",
       "\n",
       "        [[0.16470589, 0.17647059, 0.23137255],\n",
       "         [0.16078432, 0.17647059, 0.22745098],\n",
       "         [0.15686275, 0.17254902, 0.21960784],\n",
       "         ...,\n",
       "         [0.16078432, 0.16078432, 0.20392157],\n",
       "         [0.17254902, 0.16862746, 0.20784314],\n",
       "         [0.17254902, 0.17647059, 0.21176471]],\n",
       "\n",
       "        [[0.17647059, 0.19215687, 0.26274511],\n",
       "         [0.17647059, 0.19607843, 0.25490198],\n",
       "         [0.17647059, 0.19215687, 0.25490198],\n",
       "         ...,\n",
       "         [0.14901961, 0.16470589, 0.22352941],\n",
       "         [0.15294118, 0.16470589, 0.21568628],\n",
       "         [0.15686275, 0.16862746, 0.21960784]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.10196079, 0.09411765, 0.10588235],\n",
       "         [0.10588235, 0.09803922, 0.10980392],\n",
       "         [0.11764706, 0.10588235, 0.11372549],\n",
       "         ...,\n",
       "         [0.09411765, 0.09019608, 0.10588235],\n",
       "         [0.09411765, 0.09803922, 0.10980392],\n",
       "         [0.10588235, 0.10588235, 0.11372549]],\n",
       "\n",
       "        [[0.11764706, 0.11372549, 0.1254902 ],\n",
       "         [0.12941177, 0.12156863, 0.14117648],\n",
       "         [0.14509805, 0.13333334, 0.16078432],\n",
       "         ...,\n",
       "         [0.11372549, 0.1254902 , 0.13725491],\n",
       "         [0.14509805, 0.14901961, 0.15686275],\n",
       "         [0.16862746, 0.16862746, 0.17647059]],\n",
       "\n",
       "        [[0.12156863, 0.11764706, 0.12941177],\n",
       "         [0.11372549, 0.10980392, 0.13333334],\n",
       "         [0.1254902 , 0.13333334, 0.17254902],\n",
       "         ...,\n",
       "         [0.15294118, 0.15686275, 0.16078432],\n",
       "         [0.17254902, 0.17254902, 0.17647059],\n",
       "         [0.17254902, 0.18039216, 0.1882353 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.48287544, 0.38095572, 0.34520561],\n",
       "         [0.46851087, 0.36659115, 0.33802333],\n",
       "         [0.46851087, 0.36659115, 0.34520561],\n",
       "         ...,\n",
       "         [0.47569315, 0.38095572, 0.35957019],\n",
       "         [0.48287544, 0.38095572, 0.35957019],\n",
       "         [0.48287544, 0.38095572, 0.35957019]],\n",
       "\n",
       "        [[0.47569315, 0.37377344, 0.3523879 ],\n",
       "         [0.48287544, 0.37377344, 0.3523879 ],\n",
       "         [0.47569315, 0.37377344, 0.35957019],\n",
       "         ...,\n",
       "         [0.48287544, 0.38095572, 0.35957019],\n",
       "         [0.49005772, 0.38095572, 0.36675247],\n",
       "         [0.49724001, 0.38813801, 0.37393476]],\n",
       "\n",
       "        [[0.46851087, 0.37377344, 0.35957019],\n",
       "         [0.46851087, 0.36659115, 0.3523879 ],\n",
       "         [0.46132858, 0.35940886, 0.34520561],\n",
       "         ...,\n",
       "         [0.47569315, 0.38095572, 0.35957019],\n",
       "         [0.48287544, 0.38095572, 0.36675247],\n",
       "         [0.48287544, 0.37377344, 0.36675247]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.38950573, 0.30913287, 0.28774733],\n",
       "         [0.38950573, 0.30195058, 0.28056504],\n",
       "         [0.38950573, 0.30195058, 0.27338276],\n",
       "         ...,\n",
       "         [0.4541463 , 0.34504429, 0.31647647],\n",
       "         [0.47569315, 0.35222658, 0.32365876],\n",
       "         [0.4541463 , 0.33786201, 0.30929419]],\n",
       "\n",
       "        [[0.39668801, 0.30913287, 0.28774733],\n",
       "         [0.38950573, 0.30195058, 0.28056504],\n",
       "         [0.38950573, 0.30195058, 0.27338276],\n",
       "         ...,\n",
       "         [0.46132858, 0.33786201, 0.30929419],\n",
       "         [0.46851087, 0.34504429, 0.31647647],\n",
       "         [0.46851087, 0.34504429, 0.30929419]],\n",
       "\n",
       "        [[0.38950573, 0.30195058, 0.28056504],\n",
       "         [0.38950573, 0.30195058, 0.28056504],\n",
       "         [0.38950573, 0.30195058, 0.27338276],\n",
       "         ...,\n",
       "         [0.4541463 , 0.33786201, 0.3021119 ],\n",
       "         [0.46851087, 0.34504429, 0.30929419],\n",
       "         [0.46132858, 0.34504429, 0.3021119 ]]],\n",
       "\n",
       "\n",
       "       [[[0.78996103, 0.63240982, 0.55051719],\n",
       "         [0.73745128, 0.59490286, 0.56551997],\n",
       "         [0.71494711, 0.63240982, 0.64803528],\n",
       "         ...,\n",
       "         [0.59492484, 0.47488059, 0.49800744],\n",
       "         [0.34737891, 0.2798444 , 0.30297125],\n",
       "         [0.33987752, 0.27234301, 0.28046708]],\n",
       "\n",
       "        [[0.86497495, 0.70742374, 0.65553667],\n",
       "         [0.78996103, 0.63991121, 0.66303806],\n",
       "         [0.75245407, 0.63991121, 0.66303806],\n",
       "         ...,\n",
       "         [0.49740675, 0.38486389, 0.40799074],\n",
       "         [0.31737334, 0.2873458 , 0.30297125],\n",
       "         [0.32487473, 0.27234301, 0.28796847]],\n",
       "\n",
       "        [[0.94749026, 0.77493627, 0.70804641],\n",
       "         [1.        , 0.79744044, 0.74555337],\n",
       "         [0.96249304, 0.76743488, 0.7230492 ],\n",
       "         ...,\n",
       "         [0.5349137 , 0.48238198, 0.48300466],\n",
       "         [0.3548803 , 0.35485832, 0.39298796],\n",
       "         [0.31737334, 0.26484162, 0.30297125]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.39988865, 0.33235415, 0.33297682],\n",
       "         [0.38488587, 0.31735136, 0.31797404],\n",
       "         [0.37738448, 0.30234858, 0.31047265],\n",
       "         ...,\n",
       "         [0.5349137 , 0.44487503, 0.45299909],\n",
       "         [0.57992206, 0.48238198, 0.47550327],\n",
       "         [0.43739561, 0.39236528, 0.39298796]],\n",
       "\n",
       "        [[0.36238169, 0.29484719, 0.30297125],\n",
       "         [0.36238169, 0.29484719, 0.30297125],\n",
       "         [0.34737891, 0.2873458 , 0.29546986],\n",
       "         ...,\n",
       "         [0.49740675, 0.41486946, 0.40048935],\n",
       "         [0.5424151 , 0.4673792 , 0.45299909],\n",
       "         [0.43739561, 0.42237085, 0.42299352]],\n",
       "\n",
       "        [[0.36238169, 0.29484719, 0.30297125],\n",
       "         [0.3548803 , 0.29484719, 0.30297125],\n",
       "         [0.33237612, 0.2798444 , 0.29546986],\n",
       "         ...,\n",
       "         [0.54991649, 0.44487503, 0.41549213],\n",
       "         [0.54991649, 0.48238198, 0.46800188],\n",
       "         [0.42989422, 0.41486946, 0.43049492]]],\n",
       "\n",
       "\n",
       "       [[[0.3785585 , 0.3048506 , 0.33745642],\n",
       "         [0.3785585 , 0.3048506 , 0.33745642],\n",
       "         [0.36725957, 0.29355166, 0.33745642],\n",
       "         ...,\n",
       "         [0.40115638, 0.32744847, 0.37135322],\n",
       "         [0.40115638, 0.32744847, 0.37135322],\n",
       "         [0.41245531, 0.32744847, 0.37135322]],\n",
       "\n",
       "        [[0.38985744, 0.31614953, 0.36005429],\n",
       "         [0.36725957, 0.29355166, 0.32615748],\n",
       "         [0.36725957, 0.29355166, 0.32615748],\n",
       "         ...,\n",
       "         [0.38985744, 0.32744847, 0.37135322],\n",
       "         [0.40115638, 0.32744847, 0.37135322],\n",
       "         [0.41245531, 0.32744847, 0.37135322]],\n",
       "\n",
       "        [[0.35596063, 0.29355166, 0.34875535],\n",
       "         [0.35596063, 0.28225273, 0.32615748],\n",
       "         [0.36725957, 0.29355166, 0.32615748],\n",
       "         ...,\n",
       "         [0.38985744, 0.32744847, 0.37135322],\n",
       "         [0.40115638, 0.32744847, 0.37135322],\n",
       "         [0.41245531, 0.32744847, 0.37135322]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3785585 , 0.29355166, 0.34875535],\n",
       "         [0.36725957, 0.29355166, 0.33745642],\n",
       "         [0.3785585 , 0.3048506 , 0.34875535],\n",
       "         ...,\n",
       "         [0.3785585 , 0.3048506 , 0.34875535],\n",
       "         [0.36725957, 0.29355166, 0.33745642],\n",
       "         [0.38985744, 0.31614953, 0.36005429]],\n",
       "\n",
       "        [[0.38985744, 0.3048506 , 0.34875535],\n",
       "         [0.38985744, 0.3048506 , 0.34875535],\n",
       "         [0.38985744, 0.3048506 , 0.34875535],\n",
       "         ...,\n",
       "         [0.36725957, 0.29355166, 0.32615748],\n",
       "         [0.35596063, 0.29355166, 0.32615748],\n",
       "         [0.3785585 , 0.31614953, 0.36005429]],\n",
       "\n",
       "        [[0.3785585 , 0.3048506 , 0.33745642],\n",
       "         [0.36725957, 0.29355166, 0.33745642],\n",
       "         [0.36725957, 0.29355166, 0.34875535],\n",
       "         ...,\n",
       "         [0.3785585 , 0.3048506 , 0.33745642],\n",
       "         [0.36725957, 0.3048506 , 0.32615748],\n",
       "         [0.3785585 , 0.31614953, 0.34875535]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 10s 22ms/step\n",
      "Test Data accuracy:  94.37054631828978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = np.argmax(model.predict(test_data), axis=-1)\n",
    "\n",
    "#Accuracy with the test data\n",
    "print('Test Data accuracy: ',accuracy_score(test_labels, pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf = confusion_matrix(test_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model trained on the image vectors which had the bright-dark filter applied first, performed better than the model trained on images without the filter (94.4% in comparison to 93.0% test set accuracy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a further test, a copy of this notebook was ran on the HPC for 100 epochs. The model trained on the original image vector scored an accuracy of 98.7% on the test set whereas the model trained on the filtered image vector scored 98.8%. Although the improvement is small, we choose to use the image vectors with the filter applied going forward in our model. The model is saved as basic_cnn_model_filter.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
